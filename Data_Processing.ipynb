{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKWtKULE8lCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72406144-fc8b-43b9-e397-3cbfcaf9329a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Collecting recordlinkage\n",
            "  Downloading recordlinkage-0.15-py3-none-any.whl (926 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.5/926.5 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (0.8.10)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py_stringmatching\n",
            "  Downloading py_stringmatching-0.4.3.tar.gz (643 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m643.8/643.8 KB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.5.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.8/dist-packages (from recordlinkage) (1.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from recordlinkage) (1.2.0)\n",
            "Collecting jellyfish>=0.8.0\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1 in /usr/local/lib/python3.8/dist-packages (from recordlinkage) (1.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from py_stringmatching) (1.15.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (4.38.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.19.0->recordlinkage) (3.1.0)\n",
            "Building wheels for collected packages: sklearn, py_stringmatching, jellyfish\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=3ec5d797c0d0e73c4451beffcdc6218298265df3b318a28791e624d97dd5606b\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "  Building wheel for py_stringmatching (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py_stringmatching: filename=py_stringmatching-0.4.3-cp38-cp38-linux_x86_64.whl size=2683008 sha256=6cdd41d1ec64661dd643bba02602ffd7d698bbbbc39ea22ad4dfc85b378e5b7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/8e/a6/47bb1099972143b8edd2a29726e1643ef71eb66b3a1d7ec1e1\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp38-cp38-linux_x86_64.whl size=77928 sha256=05e869f6ffe0da9dbc758e0907fdfd9c1ee1a496f764c56a1b30a2b99bc842f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/c7/3c/4c83132de76359e3a429fd09c08995945ca96c5290a41651d3\n",
            "Successfully built sklearn py_stringmatching jellyfish\n",
            "Installing collected packages: sklearn, py_stringmatching, jellyfish, recordlinkage\n",
            "Successfully installed jellyfish-0.9.0 py_stringmatching-0.4.3 recordlinkage-0.15 sklearn-0.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas recordlinkage tabulate sklearn py_stringmatching matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcn6xYsRNuaJ",
        "outputId": "d9e138d9-24a5-4128-c036-ea524c2e6824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhMs_Qic8mKY",
        "outputId": "b58a9da2-3e95-4e1d-8d9f-3cc9fa159578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import re\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#folder path\n",
        "path = \"/content/gdrive/Shareddrives/bioenergitics_drive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO2g-Hkq8yro"
      },
      "outputs": [],
      "source": [
        "# Set global variables sub folders\n",
        "testing_path= \"/testing_data\"\n",
        "time_log_path = \"/time_log.csv\"\n",
        "result = \"/Colab Notebook/Results\"\n",
        "shimmer_path = \"/shimmer_data\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qg6vd81RZ19h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BORx2AZR9Nro"
      },
      "outputs": [],
      "source": [
        "# Read the data drop nulls\n",
        "time_log_df = pd.read_csv(path+testing_path+time_log_path)\n",
        "# time_log_df = time_log_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q618Jx2N9cZo"
      },
      "outputs": [],
      "source": [
        "# Remove empty records and filter out and clean the time log data\n",
        "pattern = re.compile(\"(^[a-z]+_[a-z][0-9]+_[0-9]+_[0-9]+_[0-9]+)\")\n",
        "testing_folder_names = [file for file in os.listdir(path+testing_path) if pattern.match(file)]\n",
        "list_files = [os.path.join(path+testing_path, file)  for file in os.listdir(path+testing_path) if pattern.match(file)]\n",
        "time_log_df[\"Path Name\"]=time_log_df[\"Subject\"].str.lower()+\"_v\"+time_log_df[\"Visit\"].astype(int).astype(str)+\"_\"+time_log_df[\"Date\"].str.replace(\".\",\"_\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate indexing\n",
        "\n",
        "def remove_na_values(table):\n",
        "    start_ind = table.index.tolist()[0]\n",
        "    start_ind_ls = table[~table[\"indexing\"].isna()].index.tolist()\n",
        "    if (len(start_ind_ls)>0):\n",
        "        start_ind = start_ind_ls[0]\n",
        "\n",
        "    end_ind = table.index.tolist()[-1]\n",
        "    end_ind_ls = table[~table[\"indexing\"].isna()].index.tolist()\n",
        "    if (len(end_ind_ls)>0):\n",
        "        end_ind = end_ind_ls[-1]\n",
        "    return table.loc[start_ind:end_ind,:]\n",
        "\n",
        "testing_folder_names = [file for file in os.listdir(path+testing_path) if pattern.match(file)]\n",
        "for folder in testing_folder_names:\n",
        "    # Run with one person but not sc_v1_14_10_2022\n",
        "    if folder !=\"sc_v1_14_10_2022\":\n",
        "        print(folder)\n",
        "        if not os.path.exists(path+result+\"/\"+folder+\"/Merge with Cosmed/\"):\n",
        "            os.makedirs(path+result+\"/\"+folder+\"/Merge with Cosmed/\")\n",
        "\n",
        "        for file in os.listdir(path+result+\"/\"+folder):\n",
        "            # handle Cosmed file first\n",
        "            if file ==\"cosmed_raw.csv\":\n",
        "                cosmed = pd.read_csv(path+result+\"/\"+folder+\"/cosmed_raw.csv\")\n",
        "                cosmed.insert(loc=10, column='indexing',value=\"\")\n",
        "                cosmed.reset_index(inplace=True, drop=True)\n",
        "                cosmed[\"indexing\"]=cosmed.index\n",
        "                # print(\"Cosmed\",cosmed.shape)\n",
        "                # Pick the first 30\n",
        "                # cosmed = cosmed[cosmed[\"indexing\"]<=30]\n",
        "\n",
        "        for file in os.listdir(path+result+\"/\"+folder):\n",
        "            if \"_Summary.csv\" in file:\n",
        "                summary = pd.read_csv(path+result+\"/\"+folder+\"/\"+file)\n",
        "                summary[\"Date\"]=summary[\"Time\"].str.split(\" \").str[0]\n",
        "                summary[\"t\"]= summary[\"Time\"].str.split(\" \").str[1]\n",
        "                summary[\"t\"]= summary[\"t\"].str[:8]\n",
        "                # print(\"Summary\",summary.shape)\n",
        "                summary.drop(columns=[\"Time\",\"Date\"],inplace=True)\n",
        "                summary.reset_index(inplace=True,drop=True)\n",
        "                # print(tabulate(summary_gb.head(20), headers=summary_gb.columns.tolist(), tablefmt=\"pretty\"))\n",
        "\n",
        "                # Cosmed + Zephr\n",
        "                merge_cos_zephr = summary.merge(cosmed, how='left', on=\"t\")\n",
        "                merge_cos_zephr= remove_na_values(merge_cos_zephr)\n",
        "                merge_cos_zephr.reset_index(drop=True,inplace=True)\n",
        "                merge_cos_zephr.fillna(method='ffill',inplace=True)\n",
        "                # print(\"Cosmed + Zephr\",merge_cos_zephr.shape)\n",
        "                merge_cos_zephr.to_csv(path+result+\"/\"+folder+\"/Merge with Cosmed/\"+\"cos_zeph.csv\",index=False)\n",
        "\n",
        "            elif file==\"moxy_data.csv\":\n",
        "                moxy = pd.read_csv(path+result+\"/\"+folder+\"/\"+file)\n",
        "                print(\"Moxy\",moxy.shape)\n",
        "                moxy.drop(columns=[\"mm-dd\"],inplace=True)\n",
        "                moxy_gb = moxy.groupby([\"hh:mm:ss\"]).mean()\n",
        "                moxy_gb[\"hh:mm:ss\"]=moxy_gb.index\n",
        "                moxy_gb.reset_index(inplace=True,drop=True)\n",
        "                # print(\"Moxy Group\",moxy_gb.shape)\n",
        "                # Cosmed + Moxy\n",
        "                merge_cos_moxy = moxy_gb.merge(cosmed[[\"t\",\"indexing\"]], how='left', right_on='t',left_on='hh:mm:ss')\n",
        "                merge_cos_moxy = remove_na_values(merge_cos_moxy)\n",
        "                merge_cos_moxy.reset_index(drop=True,inplace=True)\n",
        "                merge_cos_moxy.drop(columns=[\"t\",\"indexing\"],inplace=True)\n",
        "                # print(\"Cosmed + Moxy\",merge_cos_moxy.shape)\n",
        "                merge_cos_moxy.to_csv(path+result+\"/\"+folder+\"/Merge with Cosmed/\"+\"cos_moxy.csv\",index=False)\n",
        "        merge_cos_moxy_zephr = merge_cos_zephr.merge(merge_cos_moxy,how=\"left\",left_on=\"t\",right_on=\"hh:mm:ss\")\n",
        "\n",
        "        # print(\"Cos_Zephr\")\n",
        "        # print(tabulate(merge_cos_zephr.head(), headers=merge_cos_zephr.columns.tolist(), tablefmt=\"pretty\"))\n",
        "        # print(\"Cos_Moxy\")\n",
        "        # print(tabulate(merge_cos_moxy.head(), headers=merge_cos_moxy.columns.tolist(), tablefmt=\"pretty\"))\n",
        "        # print(\"Cos+Moxy+Zephr\",merge_cos_moxy_zephr.shape)\n",
        "        # print(\"Cos_Moxy_Zephr\")\n",
        "        # print(tabulate(merge_cos_moxy_zephr.head(), headers=merge_cos_moxy_zephr.columns.tolist(), tablefmt=\"pretty\"))\n",
        "        for file in os.listdir(path+result+\"/\"+folder):\n",
        "            if file ==\"shimmer_data\":\n",
        "                for subfile in os.listdir(path+result+\"/\"+folder+\"/\"+file):\n",
        "                    shimmer = pd.read_csv(path+result+\"/\"+folder+\"/\"+file+\"/\"+subfile)\n",
        "                    shimmer_timestamp = [i for i in shimmer.columns if \"Timestamp\" in i]\n",
        "                    if len(shimmer_timestamp)==1:\n",
        "                        shimmer[\"t\"]= shimmer[shimmer_timestamp[0]].str.split(\" \").str[1]\n",
        "                        shimmer[\"t\"]= shimmer[\"t\"].str[:8]\n",
        "                        replace_nan_shimmer = shimmer[shimmer[\"t\"].duplicated()]\n",
        "                        replace_nan_shimmer[\"t\"] = \"\"\n",
        "                        shimmer.update(replace_nan_shimmer)\n",
        "                        shimmer[\"t\"]=shimmer[\"t\"].replace(\"\",np.nan)\n",
        "                        # print(\"Shimmer\",shimmer.shape)\n",
        "\n",
        "                        # Cosmed + Shimmer\n",
        "                        zhr_shimmer_moxy_cosmed = shimmer.merge(merge_cos_moxy_zephr, how='left', on=\"t\")\n",
        "                        zhr_shimmer_moxy_cosmed.reset_index(drop=True,inplace=True)\n",
        "                        zhr_shimmer_moxy_cosmed= remove_na_values(zhr_shimmer_moxy_cosmed)\n",
        "                        zhr_shimmer_moxy_cosmed.fillna(method='ffill',inplace=True)\n",
        "                        # print(\"Cosmed + Shimmer\",zhr_shimmer_moxy_cosmed.shape)\n",
        "\n",
        "                        if not os.path.exists(path+result+\"/\"+folder+\"/Merge with Cosmed/zhr_shimmer_moxy_cosmed/\"):\n",
        "                            os.makedirs(path+result+\"/\"+folder+\"/Merge with Cosmed/zhr_shimmer_moxy_cosmed/\")\n",
        "                        zhr_shimmer_moxy_cosmed.to_csv(path+result+\"/\"+folder+\"/Merge with Cosmed/zhr_shimmer_moxy_cosmed/\"+subfile+\"_zhr_shimmer_moxy_cosmed.csv\",index=False)\n",
        "                        # print(\"Shimmer Zepher Moxy Cosmed\",zhr_shimmer_moxy_cosmed.shape)\n",
        "\n",
        "                        # print(\"All\")\n",
        "                        # print(tabulate(zhr_shimmer_moxy_cosmed.head(), headers=zhr_shimmer_moxy_cosmed.columns.tolist(), tablefmt=\"pretty\"))\n",
        "\n",
        "        print(\"\")"
      ],
      "metadata": {
        "id": "3BeIGZj2o-Om",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afefe51-934f-4a69-f0ce-76cdd23e4e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sg_v1_19_10_2022\n",
            "Moxy (7955, 7)\n",
            "\n",
            "cc_v1_21_10_2022\n",
            "Moxy (8556, 7)\n",
            "\n",
            "gd_v1_24_10_2022\n",
            "Moxy (9149, 7)\n",
            "\n",
            "ck_v1_24_10_2022\n",
            "Moxy (5101, 7)\n",
            "\n",
            "cc_v2_25_10_2022\n",
            "Moxy (7171, 7)\n",
            "\n",
            "gd_v2_26_10_2022\n",
            "Moxy (6637, 7)\n",
            "\n",
            "sg_v2_24_10_2022\n",
            "Moxy (4898, 7)\n",
            "\n",
            "ck_v2_26_10_2022\n",
            "Moxy (7396, 7)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iVRNxy6gFf-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ntc8zsauAVkG"
      },
      "outputs": [],
      "source": [
        "#Handle time conversion for shimmer files\n",
        "# time_log_shimmer = time_log_df[time_log_df[\"Sensor Name\"].str.contains(\"Shimmer\")]\n",
        "# for folder in list_files:\n",
        "#     for file in os.listdir(folder+shimmer_path):\n",
        "#         if file.endswith(\".csv\"):\n",
        "#             with open(folder+shimmer_path+\"/\"+file) as f:\n",
        "#                 delimiter = f.readline()[5:-2]\n",
        "#             df = pd.read_csv(os.path.join(folder+shimmer_path,file),skiprows=1,header=[0,1],delimiter=delimiter)\n",
        "#             df.columns = df.columns.map('_'.join)\n",
        "#             # Remove columns having all NaN values\n",
        "#             df.dropna(axis=1, how='all',inplace=True)\n",
        "#             # Find all columns having timestam\n",
        "#             timestamp_cols = [i for i in df.columns if \"Timestamp\" in i]\n",
        "#             # Convert the timestamp columns to time\n",
        "#             for col in timestamp_cols:\n",
        "#                 try:\n",
        "#                     df[col] = df[col].apply(lambda x: datetime.fromtimestamp(x/ 1000))\n",
        "#                 except TypeError as ex:\n",
        "#                     print(ex)\n",
        "#                     print(folder+shimmer_path+\"/\"+file)\n",
        "#             person_folder = (folder.split(\"/\")[-1])\n",
        "#             if not os.path.exists(path+result+\"/\"+person_folder+shimmer_path):\n",
        "#                 os.makedirs(path+result+\"/\"+person_folder+shimmer_path)\n",
        "#             df.to_csv(path+result+\"/\"+person_folder+shimmer_path+\"/\"+file,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuuLA9xaBGDH"
      },
      "outputs": [],
      "source": [
        "# # Set UTC for the time count of sensors for COSMED_RAW.xlsx\n",
        "# cosmed_df = time_log_df[time_log_df[\"File Name\"]==\"cosmed_raw.xlsx\"]\n",
        "# def convert_time_earlier_zephr(row):\n",
        "#     element = datetime.strptime(row,\"%d/%m/%Y  %H:%M:%S.%f\")\n",
        "#     result = element-timedelta(hours=1)\n",
        "#     return str(result)\n",
        "\n",
        "# def convert_time_earlier_moxy(row):\n",
        "#     if str(row)==\"nan\":\n",
        "#         return str(row)\n",
        "#     element = datetime.strptime(str(row),\"%H:%M:%S\")\n",
        "#     result = element-timedelta(hours=1)\n",
        "#     return str(result)\n",
        "\n",
        "# def convert_time(row,timestamp):\n",
        "#     time_token = str(row).split(\":\")\n",
        "#     x =  timestamp+timedelta(minutes=int(time_token[1]),seconds=int(time_token[2]),hours=int(time_token[0]))\n",
        "#     return x.time()\n",
        "\n",
        "# # Handle COSMED_RAW, Zephr and moxy_daa.csv files\n",
        "# for folder in list_files:\n",
        "#     for file in os.listdir(folder):\n",
        "#         if file==\"cosmed_raw.xlsx\":\n",
        "#             df = pd.read_excel(os.path.join(folder,file))\n",
        "#             df.dropna(subset=[\"t\"],inplace=True)\n",
        "#             df = df[df[\"t\"]!=\"s\"]\n",
        "#             start_time_df = cosmed_df[cosmed_df[\"Path Name\"]==folder.split(\"/\")[-1].replace(\"2022\",\"22\")]\n",
        "#             # print(folder)\n",
        "#             # print(cosmed_df)\n",
        "#             # print(start_time_df.head())\n",
        "#             if start_time_df.shape[0]>0:\n",
        "#                 start_time= (start_time_df[\"Date\"].str.replace(\".\",\"-\")+\" \"+start_time_df[\"Start Time\"]).tolist()[0]\n",
        "#                 start_time_token = (start_time.split(\"-\"))\n",
        "#                 start_time_token[-1] = \"20\"+start_time_token[-1]\n",
        "#                 start_time = \"-\".join(start_time_token)\n",
        "#                 timestamp = datetime.strptime(start_time, \"%d-%m-%Y %H:%M:%S\")\n",
        "#                 new_t = df[\"t\"].apply(lambda x: convert_time(x,timestamp))\n",
        "#                 df.update(new_t)\n",
        "#             person_folder = (folder.split(\"/\")[-1])\n",
        "#             if not os.path.exists(path+result+\"/\"+person_folder):\n",
        "#                 os.makedirs(path+result+\"/\"+person_folder)\n",
        "#             df.to_csv(path+result+\"/\"+person_folder+\"/\"+file.replace(\"xlsx\",\"csv\"),index=False)\n",
        "#         if file ==\"zephr\":\n",
        "#             person_folder = (folder.split(\"/\")[-1])\n",
        "#             for subfile in os.listdir(folder+\"/\"+file):\n",
        "#                 if \"_Summary.csv\" in subfile:\n",
        "#                     df_zephr = pd.read_csv(folder+\"/\"+file+\"/\"+subfile)\n",
        "#                     df_zephr[\"Time\"]=df_zephr[\"Time\"].apply(lambda x: convert_time_earlier_zephr(x))\n",
        "#                     if not os.path.exists(path+result+\"/\"+person_folder):\n",
        "#                         os.makedirs(path+result+\"/\"+person_folder)\n",
        "#                     df_zephr.to_csv(path+result+\"/\"+person_folder+\"/\"+subfile,index=False)\n",
        "#         if file==\"moxy_data.csv\":\n",
        "#                 person_folder = (folder.split(\"/\")[-1])\n",
        "#                 df_moxy = pd.read_csv(folder+\"/\"+file,skiprows=2,header=[1])\n",
        "#                 df_moxy[\"hh:mm:ss\"]=df_moxy[\"hh:mm:ss\"].apply(lambda x: convert_time_earlier_moxy(x))\n",
        "#                 df_moxy[\"hh:mm:ss\"]=df_moxy[\"hh:mm:ss\"].str.split(\" \").str[-1]\n",
        "#                 if not os.path.exists(path+result+\"/\"+person_folder):\n",
        "#                     os.makedirs(path+result+\"/\"+person_folder)\n",
        "#                 df_moxy.to_csv(path+result+\"/\"+person_folder+\"/\"+file,index=False)\n",
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}